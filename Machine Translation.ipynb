{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426ff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ea663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  \n",
    "epochs = 100  \n",
    "latent_dim = 256 .\n",
    "num_samples = 10000 \n",
    "n_features = 50\n",
    "data_path = \"mar.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b632b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a91ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9093aa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 85\n",
      "Max sequence length for inputs: 19\n",
      "Max sequence length for outputs: 42\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b730afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1286af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26f8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce54d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a40c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 45s 324ms/step - loss: 1.5547 - accuracy: 0.6424 - val_loss: 1.5138 - val_accuracy: 0.6104\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 38s 300ms/step - loss: 1.1409 - accuracy: 0.7089 - val_loss: 1.1936 - val_accuracy: 0.6900\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.9111 - accuracy: 0.7585 - val_loss: 1.0511 - val_accuracy: 0.7173\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.8076 - accuracy: 0.7804 - val_loss: 0.9715 - val_accuracy: 0.7341\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.7430 - accuracy: 0.7942 - val_loss: 0.8992 - val_accuracy: 0.7519\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 38s 307ms/step - loss: 0.6880 - accuracy: 0.8089 - val_loss: 0.8555 - val_accuracy: 0.7658\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.6468 - accuracy: 0.8200 - val_loss: 0.8344 - val_accuracy: 0.7700\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.6102 - accuracy: 0.8293 - val_loss: 0.8015 - val_accuracy: 0.7780\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.5782 - accuracy: 0.8383 - val_loss: 0.7642 - val_accuracy: 0.7894\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.5499 - accuracy: 0.8461 - val_loss: 0.7551 - val_accuracy: 0.7917\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.5250 - accuracy: 0.8529 - val_loss: 0.7356 - val_accuracy: 0.7960\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.5019 - accuracy: 0.8588 - val_loss: 0.7499 - val_accuracy: 0.7963\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.4808 - accuracy: 0.8642 - val_loss: 0.7061 - val_accuracy: 0.8058\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 38s 304ms/step - loss: 0.4606 - accuracy: 0.8697 - val_loss: 0.7096 - val_accuracy: 0.8050\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.4431 - accuracy: 0.8740 - val_loss: 0.6958 - val_accuracy: 0.8097\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 36s 292ms/step - loss: 0.4248 - accuracy: 0.8788 - val_loss: 0.6973 - val_accuracy: 0.8108\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.4086 - accuracy: 0.8834 - val_loss: 0.6922 - val_accuracy: 0.8098\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 0.3928 - accuracy: 0.8877 - val_loss: 0.6954 - val_accuracy: 0.8125\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.3778 - accuracy: 0.8921 - val_loss: 0.6911 - val_accuracy: 0.8152\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 37s 296ms/step - loss: 0.3641 - accuracy: 0.8954 - val_loss: 0.6915 - val_accuracy: 0.8148\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.3511 - accuracy: 0.8987 - val_loss: 0.6915 - val_accuracy: 0.8167\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 37s 299ms/step - loss: 0.3372 - accuracy: 0.9029 - val_loss: 0.7006 - val_accuracy: 0.8149\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.3240 - accuracy: 0.9062 - val_loss: 0.6998 - val_accuracy: 0.8172\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.3115 - accuracy: 0.9096 - val_loss: 0.6974 - val_accuracy: 0.8183\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.2995 - accuracy: 0.9132 - val_loss: 0.7023 - val_accuracy: 0.8186\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.2876 - accuracy: 0.9167 - val_loss: 0.7156 - val_accuracy: 0.8183\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.2767 - accuracy: 0.9199 - val_loss: 0.7172 - val_accuracy: 0.8183\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 38s 302ms/step - loss: 0.2658 - accuracy: 0.9230 - val_loss: 0.7215 - val_accuracy: 0.8185\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.2559 - accuracy: 0.9254 - val_loss: 0.7312 - val_accuracy: 0.8158\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.2454 - accuracy: 0.9284 - val_loss: 0.7336 - val_accuracy: 0.8183\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 38s 304ms/step - loss: 0.2360 - accuracy: 0.9314 - val_loss: 0.7454 - val_accuracy: 0.8164\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.2267 - accuracy: 0.9338 - val_loss: 0.7584 - val_accuracy: 0.8172\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.2180 - accuracy: 0.9362 - val_loss: 0.7692 - val_accuracy: 0.8175\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.2098 - accuracy: 0.9386 - val_loss: 0.7670 - val_accuracy: 0.8164\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.2010 - accuracy: 0.9411 - val_loss: 0.7726 - val_accuracy: 0.8163\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1936 - accuracy: 0.9428 - val_loss: 0.7886 - val_accuracy: 0.8167\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.1865 - accuracy: 0.9449 - val_loss: 0.7936 - val_accuracy: 0.8151\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1792 - accuracy: 0.9470 - val_loss: 0.8046 - val_accuracy: 0.8167\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.1724 - accuracy: 0.9489 - val_loss: 0.8194 - val_accuracy: 0.8137\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.1654 - accuracy: 0.9513 - val_loss: 0.8191 - val_accuracy: 0.8151\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.1595 - accuracy: 0.9525 - val_loss: 0.8289 - val_accuracy: 0.8139\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.1537 - accuracy: 0.9537 - val_loss: 0.8355 - val_accuracy: 0.8148\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.1483 - accuracy: 0.9555 - val_loss: 0.8482 - val_accuracy: 0.8136\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1422 - accuracy: 0.9575 - val_loss: 0.8594 - val_accuracy: 0.8133\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.1376 - accuracy: 0.9587 - val_loss: 0.8662 - val_accuracy: 0.8142\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1324 - accuracy: 0.9597 - val_loss: 0.8751 - val_accuracy: 0.8117\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1276 - accuracy: 0.9614 - val_loss: 0.8816 - val_accuracy: 0.8138\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1232 - accuracy: 0.9624 - val_loss: 0.8995 - val_accuracy: 0.8116\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.1189 - accuracy: 0.9635 - val_loss: 0.9138 - val_accuracy: 0.8125\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.1149 - accuracy: 0.9648 - val_loss: 0.9150 - val_accuracy: 0.8141\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.1108 - accuracy: 0.9658 - val_loss: 0.9260 - val_accuracy: 0.8111\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1075 - accuracy: 0.9668 - val_loss: 0.9324 - val_accuracy: 0.8130\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.1033 - accuracy: 0.9682 - val_loss: 0.9511 - val_accuracy: 0.8122\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1000 - accuracy: 0.9690 - val_loss: 0.9560 - val_accuracy: 0.8118\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 36s 287ms/step - loss: 0.0969 - accuracy: 0.9696 - val_loss: 0.9589 - val_accuracy: 0.8112\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.0934 - accuracy: 0.9708 - val_loss: 0.9716 - val_accuracy: 0.8119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.0912 - accuracy: 0.9711 - val_loss: 0.9882 - val_accuracy: 0.8107\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 35s 276ms/step - loss: 0.0877 - accuracy: 0.9722 - val_loss: 0.9879 - val_accuracy: 0.8119\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.0856 - accuracy: 0.9726 - val_loss: 0.9972 - val_accuracy: 0.8113\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0825 - accuracy: 0.9735 - val_loss: 1.0042 - val_accuracy: 0.8112\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0807 - accuracy: 0.9742 - val_loss: 1.0137 - val_accuracy: 0.8104\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.0780 - accuracy: 0.9748 - val_loss: 1.0222 - val_accuracy: 0.8111\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 37s 300ms/step - loss: 0.0757 - accuracy: 0.9753 - val_loss: 1.0264 - val_accuracy: 0.8099\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.0736 - accuracy: 0.9761 - val_loss: 1.0317 - val_accuracy: 0.8110\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0717 - accuracy: 0.9765 - val_loss: 1.0486 - val_accuracy: 0.8101\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.0703 - accuracy: 0.9769 - val_loss: 1.0575 - val_accuracy: 0.8102\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0683 - accuracy: 0.9772 - val_loss: 1.0535 - val_accuracy: 0.8113\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0665 - accuracy: 0.9780 - val_loss: 1.0718 - val_accuracy: 0.8112\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.0650 - accuracy: 0.9781 - val_loss: 1.0702 - val_accuracy: 0.8105\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.0636 - accuracy: 0.9785 - val_loss: 1.0755 - val_accuracy: 0.8112\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0620 - accuracy: 0.9791 - val_loss: 1.0876 - val_accuracy: 0.8092\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0605 - accuracy: 0.9793 - val_loss: 1.1020 - val_accuracy: 0.8107\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.0596 - accuracy: 0.9795 - val_loss: 1.0985 - val_accuracy: 0.8099\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 1.1054 - val_accuracy: 0.8107\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.0569 - accuracy: 0.9801 - val_loss: 1.1025 - val_accuracy: 0.8098\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0557 - accuracy: 0.9806 - val_loss: 1.1282 - val_accuracy: 0.8108\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.0549 - accuracy: 0.9808 - val_loss: 1.1160 - val_accuracy: 0.8110\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.0539 - accuracy: 0.9808 - val_loss: 1.1311 - val_accuracy: 0.8102\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0528 - accuracy: 0.9814 - val_loss: 1.1342 - val_accuracy: 0.8097\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0523 - accuracy: 0.9814 - val_loss: 1.1381 - val_accuracy: 0.8098\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 1.1491 - val_accuracy: 0.8106\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0504 - accuracy: 0.9818 - val_loss: 1.1474 - val_accuracy: 0.8110\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.0501 - accuracy: 0.9818 - val_loss: 1.1544 - val_accuracy: 0.8094\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0488 - accuracy: 0.9822 - val_loss: 1.1585 - val_accuracy: 0.8104\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.0482 - accuracy: 0.9823 - val_loss: 1.1582 - val_accuracy: 0.8104\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 1.1719 - val_accuracy: 0.8109\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.0466 - accuracy: 0.9827 - val_loss: 1.1712 - val_accuracy: 0.8110\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0461 - accuracy: 0.9830 - val_loss: 1.1843 - val_accuracy: 0.8090\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0455 - accuracy: 0.9833 - val_loss: 1.1753 - val_accuracy: 0.8105\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.0448 - accuracy: 0.9834 - val_loss: 1.1819 - val_accuracy: 0.8110\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0448 - accuracy: 0.9832 - val_loss: 1.1844 - val_accuracy: 0.8107\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.0440 - accuracy: 0.9834 - val_loss: 1.2031 - val_accuracy: 0.8090\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0438 - accuracy: 0.9832 - val_loss: 1.1906 - val_accuracy: 0.8105\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0432 - accuracy: 0.9833 - val_loss: 1.1988 - val_accuracy: 0.8106\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.0430 - accuracy: 0.9836 - val_loss: 1.2006 - val_accuracy: 0.8097\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.0420 - accuracy: 0.9838 - val_loss: 1.1970 - val_accuracy: 0.8106\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.0422 - accuracy: 0.9836 - val_loss: 1.2078 - val_accuracy: 0.8103\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0412 - accuracy: 0.9838 - val_loss: 1.2142 - val_accuracy: 0.8105\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.0411 - accuracy: 0.9841 - val_loss: 1.2135 - val_accuracy: 0.8101\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.0410 - accuracy: 0.9837 - val_loss: 1.2161 - val_accuracy: 0.8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001C4586F3B80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001C45889E220> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c1347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"s2s\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37cd860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefa807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f6c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: जा.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: धाव!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: धाव!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: धाव!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: धाव!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: कोण?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: वाह!\n",
      "\n",
      "-\n",
      "Input sentence: Duck!\n",
      "Decoded sentence: खाली वाका!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: फायर!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: फायर!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: वाचवा!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: वाचवा!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: उडी मार!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: उडी मार!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: उडी मार.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: उडी मार.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: थांबा!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: थांबा!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: थांबा!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: थांबा!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca5f36",
   "metadata": {},
   "source": [
    "For Machine Translation, a sequence to sequence model is used. The model consists of an encoder and a decoder each performing its specific purpose.\n",
    "\n",
    "The encoder(RNN layer) processes the input sequence and returns its own internal state. We discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n",
    "\n",
    "The decoder(also RNN layer), is trained to predict the next characters of the target sequence given previous characters of the target sequence. Encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets.\n",
    "\n",
    "We started by importing the txt file and creating 2 lists for the input texts and target texts respectively. We also created the 2 sets for the language characters.\n",
    "\n",
    "We then assigned indices to the sorted list of both input characters and target characters. Then we prepare a 3D array, for all encoder input data, decoder input data and the target data. The decoder_target_data is the same as decoder_input_data but offset by one timestep. One-hot vectorization will then be done.\n",
    "\n",
    "We then create our LSTM layer and pass our encoder tokens. However we do not use the encoder outputs. The same thing is done for the decoded tokens too and this is output is required by us. \n",
    "\n",
    "We then compile and train our model for 100 epochs which gives a good accuracy.\n",
    "\n",
    "After training,\n",
    "Encode the input sentence and retrieve the initial decoder state\n",
    "Run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character.\n",
    "Append the target character predicted and repeat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
